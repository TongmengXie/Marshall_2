{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9542df-8f5f-43ad-8f64-74397bb2e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"patent_inventors\",\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"inventors\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"name\": {\n",
    "                                            \"type\": [\"string\", \"null\"],\n",
    "                                            \"description\": \"The name of the inventor. For example, 'Acle', 'Hempnall', 'Tibenham', 'Blakeney'. If you don't know, return null.\"\n",
    "                                        },\n",
    "                                        \"sex\":{\n",
    "                                            \"type\": [\"boolean\", \"null\"],\n",
    "                                            \"description\": \"The inferred sex of the inventor. 0 for male, 1 for female. Try to guess from the first name, for example, 'Bob Dy' is 0, 'Marlyn' is 1\"\n",
    "                                        },\n",
    "                                        \"occupation\": {\n",
    "                                            \"type\": [\"string\"],\n",
    "                                            \"description\": \"Summarise the text and then infer the occupation of the inventor (Historical International Standard Classification of Occupations (HISCO) coding scheme). For example, 'Industrial machinery or tools engineers', 'Engineering technicians nec', 'Ships officers nfs', 'Newsvendors'.\"\n",
    "                                        },\n",
    "                                        \"industry\": {\n",
    "                                            \"type\": [\"string\"],\n",
    "                                            \"description\": \"The industry of the inventor's occupation belongs to (according to Industry 1950 basis, US). For example, 'Aircraft and parts', 'Retail florists', 'Postal service', 'Lady/Man of leisure', 'Common or General laborer'.\"\n",
    "                                        },\n",
    "                                            \n",
    "                                        \"address\": {\n",
    "                                            \"type\": \"object\",\n",
    "                                            \"properties\": {\n",
    "                                                \"street\": {\n",
    "                                                    \"type\": [\"string\", \"null\"],\n",
    "                                                    \"description\": \"Street address of the inventor. More detailed than city. For example, Kennett Square, Elm Ave, Maple Drive, Oak Lane. If you don't know, just leave blank\"\n",
    "                                                },\n",
    "                                                \"city\": {\n",
    "                                                    \"type\": [\"string\", \"null\"],\n",
    "                                                    \"description\": \"City of the inventor. For example: ['Attleboro, MA', ' Cincinnati, OH', 'Long Island City, NY']\"\n",
    "                                                },\n",
    "                                                \"county\": {\n",
    "                                                    \"type\": [\"string\", \"null\"],\n",
    "                                                    \"description\": \"County of the inventor. More detailed than state, but less than city\"\n",
    "                                                },\n",
    "                                                \"state\": {\n",
    "                                                        \"type\": [\"string\", \"null\"],\n",
    "                                                        \"enum\": [\n",
    "                                                            \"Alabama\", \"Illinois\", \"District of Columbia\", \"Maine\", \"Michigan\",\n",
    "                                                            \"Iowa\", \"Missouri\", \"New York\", \"Ohio\", \"Pennsylvania\", \"Florida\",\n",
    "                                                            \"Texas\", \"Wisconsin\", \"Kansas\", \"Maryland\", \"Georgia\", \"Indiana\",\n",
    "                                                            \"Utah\", \"Massachusetts\", \"Kentucky\", \"Vermont\", \"North Carolina\",\n",
    "                                                            \"Arizona\", \"Arkansas\", \"Montana\", \"Nebraska\", \"Minnesota\",\n",
    "                                                            \"Virginia\", \"Oregon\", \"Nevada\", \"New Hampshire\", \"Idaho\",\n",
    "                                                            \"Rhode Island\", \"California\", \"New Jersey\", \"Mississippi\",\n",
    "                                                            \"South Carolina\", \"Louisiana\", \"Colorado\", \"Washington\",\n",
    "                                                            \"West Virginia\", \"Tennessee\", \"Connecticut\", \"New Mexico\",\n",
    "                                                            \"South Dakota\", \"North Dakota\", \"Delaware\"\n",
    "                                                        ],\n",
    "                                                        \"description\": \"State of the inventor.\"\n",
    "                                                },\n",
    "                                                \"region\": {\n",
    "                                                    \"type\": [\"string\", \"null\"],\n",
    "                                                    \"description\": \"Census region and division of the inventor. For example: ['New England Division, 'Middle Atlantic Division', 'East North Central Division', 'West North Central Division', 'South Atlantic Division', 'East South Central Division', 'West South Central Division', 'Mountain Division',  'Pacific Divisio'].\"\n",
    "                                                },\n",
    "                                                \"foreign\": {\n",
    "                                                    \"type\": [\"boolean\", \"null\"],\n",
    "                                                    \"description\": \"True indicates froeign to the US, otherwise False.\"\n",
    "                                                },\n",
    "                                                \"full_address\": {\n",
    "                                                    \"type\": [\"string\", \"null\"],\n",
    "                                                    \"description\": \"The full address string.\"\n",
    "                                                },\n",
    "                                            },\n",
    "                                            \"required\": [\"street\", \"county\", \"city\", \"state\", \"region\", \"foreign\", \"full_address\"],\n",
    "                                            \"additionalProperties\": False\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"required\": [\"name\", \"occupation\", \"industry\", \"address\"],\n",
    "                                    \"additionalProperties\": False\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"inventors\"],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "str(response_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865df14-69fd-442a-bff3-bcf3a14c9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patent_content = 'Application filed March 20, 1889. Serial No. 303,951. (No model.) To all whom, lb may concern: Be it known that we, George Bensel and Otto Theodore Maier, both of New Orleans, in the parish of Orleans and State of Louis5 iana, have invented a new and Improved Fastening for Doors and Shutters, of which the following is a full, clear, and exact description. The object of the invention is to provide a io new and improved fastening for doors and shutters which permits of opening the doors or shutters from the outside in case of lire. The invention consists of a pin orpins supporting the lock and p'\n",
    "\n",
    "sys_mess = f'''\n",
    "You are an expert in information extraction. You have been asked to extract all information related to the name, occupation, and address for inventors in a patent document. The text is the OCR result of historical patents in the US.\n",
    "\n",
    "Please adhere to the following instructions:\n",
    "1. Return a JSON object for each inventor following the response format specified. Ensure each inventor has their properties assigned correctly.\n",
    "2. Try to populate every property for 'address' (e.g., street, city, state). If any fields are missing, leave them as blank strings (\"\"). Do not guess or misassign values.\n",
    "3. Summarize the text and infer 'occupation' and 'industry' fields using the Historical International Standard Classification of Occupations (HISCO) coding scheme.\n",
    "\n",
    "Please extract the following details for each inventor in JSON format. If any information is missing, leave it blank. \n",
    "Here is the schema for JSON structure:\n",
    "{response_format}\n",
    "'''\n",
    "\n",
    "user_input = f'''\n",
    "Now, based on the following patent description, extract the relevant details:\n",
    "{patent_content}\n",
    "'''\n",
    "\n",
    "message = sys_mess + user_input\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18709e-a40a-40a2-8ef5-2054dba6528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c3fc6-75f8-4b59-a49d-e5af32724e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "message\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a300e2c-7c81-4526-990b-09e3c1db3a42",
   "metadata": {},
   "source": [
    "blocker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035c9fc-6e67-4f4f-8f48-2c876f7b5ef1",
   "metadata": {},
   "source": [
    "## Llama3_1\n",
    "Gated, need huggingface logging in\n",
    "\n",
    "Network error, trying caching to local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd1a51-6150-4902-9188-b0eaa200a7d2",
   "metadata": {},
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d85d8-1089-43a4-86cc-fda363973803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface-hub\n",
    "# !HUGGINGFACE_TOKEN=\"your_token\" huggingface-cli login\n",
    "# !huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --cache-dir ../model/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619bba9-f02a-4190-91b3-ffb7af102ed0",
   "metadata": {},
   "source": [
    "### Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692182f8-1b0a-4cbf-ae8e-581c69c581ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv)\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "# Log in programmatically with your token\n",
    "\n",
    "login(token=HUGGINGFACE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c4bac-c431-45ca-81cf-f8c606e55da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade\n",
    "%%timeit\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,  # Add this line\n",
    "\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2763ec12-a01d-4cd1-890f-91a477fe7913",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "../model/Llama-3.1-8B-Instruct does not appear to have a file named config.json. Checkout 'https://huggingface.co/../model/Llama-3.1-8B-Instruct/tree/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Replace with your local model path\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../model/Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m     10\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mbfloat16},\n\u001b[1;32m     11\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a pirate chatbot who always responds in pirate speak!\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     17\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     18\u001b[0m ]\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     21\u001b[0m     messages,\n\u001b[1;32m     22\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/pipelines/__init__.py:805\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    803\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 805\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    806\u001b[0m         model, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, code_revision\u001b[38;5;241m=\u001b[39mcode_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m    807\u001b[0m     )\n\u001b[1;32m    808\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    810\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:976\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    974\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 976\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    977\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    978\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    690\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    691\u001b[0m         configuration_file,\n\u001b[1;32m    692\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    693\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    694\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    695\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    696\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    697\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    698\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    699\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    700\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    701\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:373\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    374\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    376\u001b[0m         )\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: ../model/Llama-3.1-8B-Instruct does not appear to have a file named config.json. Checkout 'https://huggingface.co/../model/Llama-3.1-8B-Instruct/tree/None' for available files."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Replace with your local model path\n",
    "model_id = \"../model/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6706064-4638-4c56-a49f-f941779d1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id = \"../model/Llama-3.1-8B-Instruct/models--meta-llama--Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "def generate_text(prompt, sys_mess, user_input):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": sys_mess},\n",
    "    {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=2048,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][-1]\n",
    "\n",
    "cpu_counts= os.cpu_count()\n",
    "# Function to parallelize model inference across multiple prompts\n",
    "def parallel_inference(prompts, num_workers=64):\n",
    "    with mp.Pool(processes=num_workers) as pool:\n",
    "        results = pool.star_map(generate_text, prompts, sys_mess, user_input)\n",
    "    return results\n",
    "    \n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f400fcc-d517-4e1d-9469-6fbd5696fa86",
   "metadata": {},
   "source": [
    "## GPTNeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0aa36f-9eac-4f9d-8748-0f7cec31c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2449a3b-d700-4a2f-aeb7-ad02334fb4aa",
   "metadata": {},
   "source": [
    "\\nFor example:\\n{\\n    \"inventors\": [\\n        {\\n            \"name\": \"Acle\",\\n            \"industry\": \"Industry 1950\",\\n            \"occupation\": \"Engineering technicians nfs\"\\n        },\\n        {\\n            \"name\": \"Bob Dy\",\\n            \"industry\": \"Aircraft and parts\",\\n            \"occupation\": \"Engineering technicians nec\"\\n        },\\n        {\\n            \"name\": \"Marlyn\",\\n            \"industry\": \"Newsvendors\",\\n            \"occupation\": \"Ships officers nfs\"\\n        },\\n        {\\n            \"name\": \"Tibenham\",\\n            \"industry\": \"Retail florists\",\\n            \"occupation\": \"Lady/Man of leisure\"\\n        },\\n        {\\n            \"name\": \"Blakeney\",\\n            \"industry\": \"Furnishing\",\\n            \"occupation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fdc6b-581f-4910-bc18-1cd5eb41305d",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fb874-0ab8-48f6-a635-49ace005627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "sequence_ids = model.generate(input_ids)\n",
    "sequences = tokenizer.batch_decode(sequence_ids)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0361d0c-6faa-4c4a-8ad7-fcaf80589386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "# task_prefix = \"translate English to German: \"\n",
    "# # use different length sentences to test batching\n",
    "# sentences = [\"The house is wonderful.\", \"I like to work in NYC.\"]\n",
    "# inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# output_sequences = model.generate(\n",
    "#     input_ids=inputs[\"input_ids\"],\n",
    "#     attention_mask=inputs[\"attention_mask\"],\n",
    "#     do_sample=False,  # disable sampling to test if batching affects output\n",
    "# )\n",
    "\n",
    "# print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c19305-ed19-44ba-8d7f-375feb45f009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_prefix = sys_mess\n",
    "# use different length sentences to test batching\n",
    "sentence = user_input\n",
    "\n",
    "inputs = tokenizer(task_prefix + sentence, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    "    max_length=2048\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "870ec680-8894-43ae-8a5e-66c2f48c6b3c",
   "metadata": {},
   "source": [
    "[\": '':'string', 'null'], 'description': 'The full address string.','required': ['street', 'county','state','state','state','state','state','region','region', 'description': 'the full address string', 'description': 'True\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
